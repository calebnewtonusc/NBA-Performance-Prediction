{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# 05. Random Forest Ensemble Model\n",
    "\n",
    "This notebook implements a Random Forest classifier - an ensemble of decision trees.\n",
    "\n",
    "## Objectives\n",
    "- Build a Random Forest classifier\n",
    "- Tune number of trees and other hyperparameters\n",
    "- Analyze feature importance across the ensemble\n",
    "- Compare to single decision tree\n",
    "- Measure out-of-bag (OOB) error\n",
    "\n",
    "## Why Random Forests?\n",
    "- **Reduces Overfitting**: Averages many trees\n",
    "- **More Robust**: Less sensitive to noise\n",
    "- **Better Generalization**: Usually outperforms single trees\n",
    "- **Feature Importance**: More reliable estimates\n",
    "- **Out-of-Bag**: Built-in validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.data_processing.cleaning import DataCleaner\n",
    "from src.data_processing.game_features import GameFeatureEngineer\n",
    "from src.data_processing.dataset_builder import DatasetBuilder\n",
    "from src.models.random_forest_model import GameRandomForest\n",
    "from src.models.decision_tree_model import GameDecisionTree\n",
    "from src.evaluation.model_comparison import ModelComparison\n",
    "from src.utils.data_loader import load_games_as_dataframe\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"âœ“ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## 1. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare\n",
    "try:\n",
    "    games_df = load_games_as_dataframe(season=2023)\n",
    "except:\n",
    "    from scripts.generate_sample_data import generate_sample_games\n",
    "    games_df = pd.DataFrame(generate_sample_games(200))\n",
    "\n",
    "cleaner = DataCleaner()\n",
    "games_df = cleaner.clean_game_data(games_df)\n",
    "\n",
    "engineer = GameFeatureEngineer()\n",
    "features_df = engineer.create_game_features(games_df)\n",
    "\n",
    "builder = DatasetBuilder()\n",
    "dataset = builder.create_dataset(\n",
    "    df=features_df,\n",
    "    target_column='home_win',\n",
    "    date_column='date',\n",
    "    split_method='time',\n",
    "    scale_features=False,\n",
    "    exclude_columns=['game_id', 'home_team_id', 'away_team_id', 'home_score', 'away_score']\n",
    ")\n",
    "\n",
    "print(f\"Dataset ready: {len(dataset['X_train'])} training samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-header",
   "metadata": {},
   "source": [
    "## 2. Train Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-rf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "rf_model = GameRandomForest()\n",
    "\n",
    "print(\"Training Random Forest...\")\n",
    "print(\"This trains an ensemble of decision trees with bootstrap sampling\")\n",
    "\n",
    "train_metrics = rf_model.train(\n",
    "    dataset['X_train'],\n",
    "    dataset['y_train'],\n",
    "    dataset['X_val'],\n",
    "    dataset['y_val'],\n",
    "    tune_hyperparameters=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BEST HYPERPARAMETERS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Number of Trees: {rf_model.model.n_estimators}\")\n",
    "print(f\"Max Depth: {rf_model.model.max_depth}\")\n",
    "print(f\"Min Samples Split: {rf_model.model.min_samples_split}\")\n",
    "print(f\"Max Features: {rf_model.model.max_features}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "for metric, value in train_metrics.items():\n",
    "    print(f\"{metric:20s}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oob-header",
   "metadata": {},
   "source": [
    "## 3. Out-of-Bag (OOB) Score\n",
    "\n",
    "Random Forests have a built-in validation mechanism using OOB samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oob-score",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get OOB score if available\n",
    "if hasattr(rf_model.model, 'oob_score_'):\n",
    "    print(f\"Out-of-Bag Score: {rf_model.model.oob_score_:.4f}\")\n",
    "    print(\"\\nOOB provides an unbiased estimate without needing a separate validation set!\")\n",
    "else:\n",
    "    print(\"OOB scoring not enabled (set oob_score=True when training)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-header",
   "metadata": {},
   "source": [
    "## 4. Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = rf_model.evaluate(dataset['X_test'], dataset['y_test'])\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEST SET PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"{metric:20s}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "importance-header",
   "metadata": {},
   "source": [
    "## 5. Feature Importance (Ensemble Average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get aggregated feature importance\n",
    "importance_df = rf_model.get_feature_importance(dataset['feature_names'])\n",
    "\n",
    "# Plot\n",
    "top_features = importance_df.head(15)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Mean Decrease in Impurity')\n",
    "plt.title('Top 15 Features - Random Forest (Averaged over all trees)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 Features:\")\n",
    "print(top_features.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare-header",
   "metadata": {},
   "source": [
    "## 6. Compare to Single Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-dt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train single decision tree\n",
    "dt_model = GameDecisionTree()\n",
    "dt_model.train(\n",
    "    dataset['X_train'],\n",
    "    dataset['y_train'],\n",
    "    dataset['X_val'],\n",
    "    dataset['y_val'],\n",
    "    tune_hyperparameters=True\n",
    ")\n",
    "\n",
    "# Compare\n",
    "comparison = ModelComparison(task_type='classification')\n",
    "comparison.add_model('Decision Tree', dt_model, dataset['X_test'], dataset['y_test'])\n",
    "comparison.add_model('Random Forest', rf_model, dataset['X_test'], dataset['y_test'])\n",
    "\n",
    "results = comparison.compare_all()\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RANDOM FOREST vs DECISION TREE\")\n",
    "print(\"=\"*60)\n",
    "print(results)\n",
    "\n",
    "best_model, _ = comparison.get_best_model()\n",
    "print(f\"\\nâœ“ Best Model: {best_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voting-header",
   "metadata": {},
   "source": [
    "## 7. Understanding Ensemble Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prediction-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction confidence\n",
    "test_proba = rf_model.predict_proba(dataset['X_test'])\n",
    "test_pred = rf_model.predict(dataset['X_test'])\n",
    "\n",
    "# Confidence distribution\n",
    "confidence = np.max(test_proba, axis=1)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(confidence, bins=20, edgecolor='black')\n",
    "plt.xlabel('Prediction Confidence')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Prediction Confidence')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "correct = test_pred == dataset['y_test']\n",
    "plt.hist([confidence[correct], confidence[~correct]], \n",
    "         bins=20, label=['Correct', 'Incorrect'], alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Confidence')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Confidence: Correct vs Incorrect Predictions')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean confidence: {confidence.mean():.3f}\")\n",
    "print(f\"High confidence (>0.7): {(confidence > 0.7).sum()} predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-header",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "### Random Forest Advantages\n",
    "- âœ“ More accurate than single decision tree\n",
    "- âœ“ Reduces overfitting through averaging\n",
    "- âœ“ More stable and robust predictions\n",
    "- âœ“ Reliable feature importance\n",
    "- âœ“ Out-of-bag validation\n",
    "\n",
    "### Performance Improvement\n",
    "Random Forest typically improves accuracy by 2-5% over single decision trees.\n",
    "\n",
    "### Key Insights\n",
    "- Ensemble methods combine multiple weak learners into a strong learner\n",
    "- Bootstrap sampling and feature randomness reduce correlation between trees\n",
    "- Majority voting produces more reliable predictions\n",
    "\n",
    "### Next Steps\n",
    "â†’ Notebook 06: Compare ALL models (Logistic Regression, Decision Tree, Random Forest)\n",
    "\n",
    "ðŸŒ²ðŸŒ²ðŸŒ² A forest is better than a single tree!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
