{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# 07. Linear Regression for Player Points Prediction\n",
    "\n",
    "This notebook demonstrates building regression models to predict individual player statistics.\n",
    "\n",
    "## Objectives\n",
    "- Predict player points per game\n",
    "- Build Linear, Ridge, and Lasso regression models\n",
    "- Perform residual analysis\n",
    "- Check regression assumptions\n",
    "- Compare regularization techniques\n",
    "\n",
    "## Regression vs Classification\n",
    "- **Classification** (Notebooks 03-06): Predict categories (Win/Loss)\n",
    "- **Regression** (This notebook): Predict continuous values (Points)\n",
    "\n",
    "## Why Regression for Player Stats?\n",
    "- Predict exact point totals\n",
    "- Understand which factors drive scoring\n",
    "- Identify over/under-performing players\n",
    "- Fantasy sports applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.data_processing.cleaning import DataCleaner\n",
    "from src.data_processing.player_features import PlayerFeatureEngineer\n",
    "from src.data_processing.dataset_builder import DatasetBuilder\n",
    "from src.models.linear_regression_model import PlayerLinearRegression\n",
    "from src.models.ridge_lasso_regression import PlayerRidgeRegression, PlayerLassoRegression\n",
    "from src.evaluation.model_comparison import ModelComparison\n",
    "from src.evaluation.metrics import RegressionMetrics\n",
    "from src.utils.data_loader import load_player_stats_as_dataframe\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"‚úì Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data-header",
   "metadata": {},
   "source": [
    "## 1. Load Player Statistics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load player stats\n",
    "try:\n",
    "    stats_df = load_player_stats_as_dataframe(season=2023)\n",
    "    print(f\"Loaded {len(stats_df)} player stat records\")\n",
    "except:\n",
    "    print(\"Loading sample data...\")\n",
    "    from scripts.generate_sample_data import generate_sample_player_stats\n",
    "    stats_df = pd.DataFrame(generate_sample_player_stats(100))\n",
    "    print(f\"Generated {len(stats_df)} sample player stats\")\n",
    "\n",
    "print(f\"\\nDataset shape: {stats_df.shape}\")\n",
    "print(f\"\\nSample data:\")\n",
    "stats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-target",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore target variable (points)\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(stats_df['pts'], bins=30, edgecolor='black')\n",
    "plt.xlabel('Points')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Player Points')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(stats_df['pts'])\n",
    "plt.ylabel('Points')\n",
    "plt.title('Points Distribution (Box Plot)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Points Statistics:\")\n",
    "print(stats_df['pts'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-header",
   "metadata": {},
   "source": [
    "## 2. Clean and Engineer Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-engineer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "cleaner = DataCleaner()\n",
    "stats_df = cleaner.clean_player_stats(stats_df)\n",
    "print(f\"After cleaning: {len(stats_df)} records\")\n",
    "\n",
    "# Engineer features\n",
    "engineer = PlayerFeatureEngineer()\n",
    "features_df = engineer.create_player_features(\n",
    "    stats_df,\n",
    "    include_target=True,\n",
    "    target_column='pts'\n",
    ")\n",
    "\n",
    "print(f\"\\nCreated {len(features_df.columns)} features\")\n",
    "print(f\"Features: {features_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-header",
   "metadata": {},
   "source": [
    "## 3. Create Train/Val/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-splits",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset\n",
    "builder = DatasetBuilder()\n",
    "dataset = builder.create_dataset(\n",
    "    df=features_df,\n",
    "    target_column='target',  # Points are stored as 'target'\n",
    "    date_column='game_date',\n",
    "    split_method='time',\n",
    "    scale_features=True,  # Important for regression!\n",
    "    exclude_columns=['player_id', 'game_id']\n",
    ")\n",
    "\n",
    "print(\"Dataset splits:\")\n",
    "print(f\"  Training:   {len(dataset['X_train'])} samples\")\n",
    "print(f\"  Validation: {len(dataset['X_val'])} samples\")\n",
    "print(f\"  Testing:    {len(dataset['X_test'])} samples\")\n",
    "print(f\"\\nFeatures: {dataset['X_train'].shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-header",
   "metadata": {},
   "source": [
    "## 4. Linear Regression (No Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-linear",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train linear regression\n",
    "linear_model = PlayerLinearRegression()\n",
    "\n",
    "print(\"Training Linear Regression...\")\n",
    "linear_model.train(\n",
    "    dataset['X_train'],\n",
    "    dataset['y_train'],\n",
    "    dataset['X_val'],\n",
    "    dataset['y_val']\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_metrics = linear_model.evaluate(dataset['X_test'], dataset['y_test'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LINEAR REGRESSION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"{metric:20s}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residuals-header",
   "metadata": {},
   "source": [
    "## 5. Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residual-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check regression assumptions\n",
    "linear_model.check_assumptions(dataset['X_test'], dataset['y_test'])\n",
    "plt.show()\n",
    "\n",
    "# Analyze residuals\n",
    "residuals_stats = linear_model.analyze_residuals(dataset['X_test'], dataset['y_test'])\n",
    "print(\"\\nResidual Statistics:\")\n",
    "for key, value in residuals_stats.items():\n",
    "    print(f\"{key:15s}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ridge-header",
   "metadata": {},
   "source": [
    "## 6. Ridge Regression (L2 Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-ridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Ridge regression\n",
    "ridge_model = PlayerRidgeRegression()\n",
    "\n",
    "print(\"Training Ridge Regression with L2 regularization...\")\n",
    "ridge_model.train(\n",
    "    dataset['X_train'],\n",
    "    dataset['y_train'],\n",
    "    dataset['X_val'],\n",
    "    dataset['y_val'],\n",
    "    tune_alpha=True\n",
    ")\n",
    "\n",
    "ridge_metrics = ridge_model.evaluate(dataset['X_test'], dataset['y_test'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RIDGE REGRESSION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best alpha: {ridge_model.model.alpha}\")\n",
    "for metric, value in ridge_metrics.items():\n",
    "    print(f\"{metric:20s}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasso-header",
   "metadata": {},
   "source": [
    "## 7. Lasso Regression (L1 Regularization + Feature Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-lasso",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Lasso regression\n",
    "lasso_model = PlayerLassoRegression()\n",
    "\n",
    "print(\"Training Lasso Regression with L1 regularization...\")\n",
    "lasso_model.train(\n",
    "    dataset['X_train'],\n",
    "    dataset['y_train'],\n",
    "    dataset['X_val'],\n",
    "    dataset['y_val'],\n",
    "    tune_alpha=True\n",
    ")\n",
    "\n",
    "lasso_metrics = lasso_model.evaluate(dataset['X_test'], dataset['y_test'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LASSO REGRESSION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best alpha: {lasso_model.model.alpha}\")\n",
    "for metric, value in lasso_metrics.items():\n",
    "    print(f\"{metric:20s}: {value:.4f}\")\n",
    "\n",
    "# Feature selection\n",
    "selected_features = lasso_model.get_selected_features()\n",
    "print(f\"\\n‚úì Lasso selected {len(selected_features)} features (out of {dataset['X_train'].shape[1]})\")\n",
    "print(f\"Selected features: {selected_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare-header",
   "metadata": {},
   "source": [
    "## 8. Compare All Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "comparison = ModelComparison(task_type='regression')\n",
    "comparison.add_model('Linear Regression', linear_model, dataset['X_test'], dataset['y_test'])\n",
    "comparison.add_model('Ridge Regression', ridge_model, dataset['X_test'], dataset['y_test'])\n",
    "comparison.add_model('Lasso Regression', lasso_model, dataset['X_test'], dataset['y_test'])\n",
    "\n",
    "results = comparison.compare_all()\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGRESSION MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(results)\n",
    "\n",
    "best_name, best_model = comparison.get_best_model(metric='mae')\n",
    "print(f\"\\n‚úì Best Model (by MAE): {best_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-header",
   "metadata": {},
   "source": [
    "## 9. Prediction Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "predictions-vs-actual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions vs Actual for all models\n",
    "metrics_helper = RegressionMetrics()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (name, model) in enumerate([\n",
    "    ('Linear', linear_model),\n",
    "    ('Ridge', ridge_model),\n",
    "    ('Lasso', lasso_model)\n",
    "]):\n",
    "    y_pred = model.predict(dataset['X_test'])\n",
    "    plt.sca(axes[idx])\n",
    "    metrics_helper.plot_predictions_vs_actual(dataset['y_test'], y_pred)\n",
    "    axes[idx].set_title(f'{name} Regression\\nPredictions vs Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residual-plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (name, model) in enumerate([\n",
    "    ('Linear', linear_model),\n",
    "    ('Ridge', ridge_model),\n",
    "    ('Lasso', lasso_model)\n",
    "]):\n",
    "    y_pred = model.predict(dataset['X_test'])\n",
    "    plt.sca(axes[idx])\n",
    "    metrics_helper.plot_residuals(dataset['y_test'], y_pred)\n",
    "    axes[idx].set_title(f'{name} Regression\\nResiduals')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-importance-header",
   "metadata": {},
   "source": [
    "## 10. Feature Importance (Linear Coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from linear model\n",
    "importance_df = linear_model.get_feature_importance(dataset['feature_names'])\n",
    "\n",
    "# Plot top features\n",
    "top_features = importance_df.head(15)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['green' if x > 0 else 'red' for x in top_features['coefficient']]\n",
    "plt.barh(range(len(top_features)), top_features['importance'], color=colors, alpha=0.7)\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Coefficient Magnitude')\n",
    "plt.title('Top 15 Features for Points Prediction\\n(Green = Positive, Red = Negative)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 Features:\")\n",
    "print(importance_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-header",
   "metadata": {},
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "### Model Performance Summary\n",
    "- **MAE (Mean Absolute Error)**: How many points off, on average?\n",
    "- **RMSE (Root Mean Squared Error)**: Penalizes larger errors\n",
    "- **R¬≤ Score**: How much variance is explained?\n",
    "\n",
    "### Regularization Comparison\n",
    "\n",
    "**Linear Regression:**\n",
    "- No penalty on coefficients\n",
    "- Can overfit with many features\n",
    "- Best when features are truly predictive\n",
    "\n",
    "**Ridge Regression (L2):**\n",
    "- Shrinks coefficients toward zero\n",
    "- Keeps all features\n",
    "- Good when many features are somewhat useful\n",
    "- Often best generalization\n",
    "\n",
    "**Lasso Regression (L1):**\n",
    "- Forces some coefficients to exactly zero\n",
    "- Performs feature selection\n",
    "- Good when many features are irrelevant\n",
    "- Interpretable (fewer features)\n",
    "\n",
    "### Typical Results\n",
    "- MAE: 3-5 points (predictions within ~4 points on average)\n",
    "- R¬≤: 0.6-0.8 (explaining 60-80% of variance)\n",
    "- Ridge often performs best\n",
    "\n",
    "### Key Insights\n",
    "- Recent performance is most predictive\n",
    "- Usage rate and minutes strongly correlate with points\n",
    "- Opponent defense rating matters\n",
    "- Some players more predictable than others\n",
    "\n",
    "### Applications\n",
    "- Fantasy sports lineups\n",
    "- Sports betting insights\n",
    "- Player performance tracking\n",
    "- Contract negotiations\n",
    "\n",
    "### Extensions\n",
    "- Multi-output regression for Points + Rebounds + Assists\n",
    "- Time series models for trends\n",
    "- Player-specific models\n",
    "- Advanced features (opponent matchups, rest days)\n",
    "\n",
    "üèÄ Regression models complete - ready to predict player performance!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
