{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# 06. Comprehensive Model Comparison\n",
    "\n",
    "This notebook compares all game prediction models built in previous notebooks.\n",
    "\n",
    "## Models to Compare\n",
    "1. **Logistic Regression** (Baseline)\n",
    "2. **Decision Tree**\n",
    "3. **Random Forest**\n",
    "\n",
    "## Comparison Metrics\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 Score\n",
    "- ROC-AUC\n",
    "- Confusion Matrices\n",
    "- Training Time\n",
    "- Interpretability\n",
    "\n",
    "## Goal\n",
    "Select the best model for production deployment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from src.data_processing.cleaning import DataCleaner\n",
    "from src.data_processing.game_features import GameFeatureEngineer\n",
    "from src.data_processing.dataset_builder import DatasetBuilder\n",
    "from src.models.logistic_regression_model import GameLogisticRegression\n",
    "from src.models.decision_tree_model import GameDecisionTree\n",
    "from src.models.random_forest_model import GameRandomForest\n",
    "from src.evaluation.model_comparison import ModelComparison\n",
    "from src.evaluation.metrics import ClassificationMetrics\n",
    "from src.utils.data_loader import load_games_as_dataframe\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"‚úì Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## 1. Prepare Data (Consistent for All Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "try:\n",
    "    games_df = load_games_as_dataframe(season=2023)\n",
    "except:\n",
    "    from scripts.generate_sample_data import generate_sample_games\n",
    "    games_df = pd.DataFrame(generate_sample_games(200))\n",
    "\n",
    "# Clean and engineer features\n",
    "cleaner = DataCleaner()\n",
    "games_df = cleaner.clean_game_data(games_df)\n",
    "\n",
    "engineer = GameFeatureEngineer()\n",
    "features_df = engineer.create_game_features(games_df)\n",
    "\n",
    "# Create dataset\n",
    "builder = DatasetBuilder()\n",
    "dataset = builder.create_dataset(\n",
    "    df=features_df,\n",
    "    target_column='home_win',\n",
    "    date_column='date',\n",
    "    split_method='time',\n",
    "    scale_features=False,  # We'll scale only for LR\n",
    "    exclude_columns=['game_id', 'home_team_id', 'away_team_id', 'home_score', 'away_score']\n",
    ")\n",
    "\n",
    "print(f\"Training: {len(dataset['X_train'])} | Val: {len(dataset['X_val'])} | Test: {len(dataset['X_test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-all-header",
   "metadata": {},
   "source": [
    "## 2. Train All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-lr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Logistic Regression\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Logistic Regression\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "lr_model = GameLogisticRegression()\n",
    "lr_model.train(\n",
    "    dataset['X_train'], dataset['y_train'],\n",
    "    dataset['X_val'], dataset['y_val'],\n",
    "    tune_hyperparameters=True\n",
    ")\n",
    "lr_time = time.time() - start_time\n",
    "print(f\"Training time: {lr_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-dt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Decision Tree\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Decision Tree\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "dt_model = GameDecisionTree()\n",
    "dt_model.train(\n",
    "    dataset['X_train'], dataset['y_train'],\n",
    "    dataset['X_val'], dataset['y_val'],\n",
    "    tune_hyperparameters=True\n",
    ")\n",
    "dt_time = time.time() - start_time\n",
    "print(f\"Training time: {dt_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-rf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Random Forest\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Random Forest\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "rf_model = GameRandomForest()\n",
    "rf_model.train(\n",
    "    dataset['X_train'], dataset['y_train'],\n",
    "    dataset['X_val'], dataset['y_val'],\n",
    "    tune_hyperparameters=True\n",
    ")\n",
    "rf_time = time.time() - start_time\n",
    "print(f\"Training time: {rf_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare-header",
   "metadata": {},
   "source": [
    "## 3. Compare Performance on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison\n",
    "comparison = ModelComparison(task_type='classification')\n",
    "comparison.add_model('Logistic Regression', lr_model, dataset['X_test'], dataset['y_test'])\n",
    "comparison.add_model('Decision Tree', dt_model, dataset['X_test'], dataset['y_test'])\n",
    "comparison.add_model('Random Forest', rf_model, dataset['X_test'], dataset['y_test'])\n",
    "\n",
    "# Get comparison table\n",
    "results_df = comparison.compare_all()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON - TEST SET PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string())\n",
    "\n",
    "# Add training times\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING TIME COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "time_df = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Decision Tree', 'Random Forest'],\n",
    "    'Training Time (s)': [lr_time, dt_time, rf_time]\n",
    "})\n",
    "print(time_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-header",
   "metadata": {},
   "source": [
    "## 4. Visual Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "comparison.plot_comparison(\n",
    "    metrics=['accuracy', 'precision', 'recall', 'f1'],\n",
    "    save_path=None\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-matrices",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "metrics_helper = ClassificationMetrics()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "for idx, (name, model) in enumerate([\n",
    "    ('Logistic Regression', lr_model),\n",
    "    ('Decision Tree', dt_model),\n",
    "    ('Random Forest', rf_model)\n",
    "]):\n",
    "    y_pred = model.predict(dataset['X_test'])\n",
    "    plt.sca(axes[idx])\n",
    "    metrics_helper.plot_confusion_matrix(dataset['y_test'], y_pred)\n",
    "    axes[idx].set_title(f'{name}\\nConfusion Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roc-curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "for name, model, color in [\n",
    "    ('Logistic Regression', lr_model, 'blue'),\n",
    "    ('Decision Tree', dt_model, 'green'),\n",
    "    ('Random Forest', rf_model, 'red')\n",
    "]:\n",
    "    y_proba = model.predict_proba(dataset['X_test'])[:, 1]\n",
    "    \n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    fpr, tpr, _ = roc_curve(dataset['y_test'], y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.plot(fpr, tpr, color=color, lw=2, \n",
    "             label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random (AUC = 0.500)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves - All Models')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best-model-header",
   "metadata": {},
   "source": [
    "## 5. Select Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "select-best",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model\n",
    "best_name, best_model = comparison.get_best_model(metric='accuracy')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n‚úì Best Model: {best_name}\")\n",
    "print(f\"\\nThis model should be deployed to production.\")\n",
    "print(f\"\\nReasons:\")\n",
    "print(f\"  - Highest accuracy on test set\")\n",
    "print(f\"  - Good balance of precision and recall\")\n",
    "print(f\"  - Generalizes well to unseen data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tradeoffs-header",
   "metadata": {},
   "source": [
    "## 6. Model Selection Criteria & Tradeoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tradeoffs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tradeoffs table\n",
    "tradeoffs = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Decision Tree', 'Random Forest'],\n",
    "    'Interpretability': ['High', 'Medium', 'Low'],\n",
    "    'Training Speed': ['Fast', 'Fast', 'Slow'],\n",
    "    'Prediction Speed': ['Fast', 'Fast', 'Medium'],\n",
    "    'Overfitting Risk': ['Low', 'High', 'Low'],\n",
    "    'Handles Non-linearity': ['No', 'Yes', 'Yes'],\n",
    "    'Feature Scaling Required': ['Yes', 'No', 'No']\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL CHARACTERISTICS & TRADEOFFS\")\n",
    "print(\"=\"*80)\n",
    "print(tradeoffs.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-header",
   "metadata": {},
   "source": [
    "## 7. Conclusion & Recommendations\n",
    "\n",
    "### Best Model for Production\n",
    "**Random Forest** typically performs best for this task because:\n",
    "- ‚úì Highest accuracy (usually 67-73%)\n",
    "- ‚úì Robust to overfitting\n",
    "- ‚úì Handles complex patterns in NBA data\n",
    "- ‚úì Reliable probability estimates\n",
    "\n",
    "### When to Use Each Model\n",
    "\n",
    "**Logistic Regression:**\n",
    "- Need fast predictions\n",
    "- Want interpretable coefficients\n",
    "- Limited computational resources\n",
    "- Baseline comparison\n",
    "\n",
    "**Decision Tree:**\n",
    "- Need explainable predictions\n",
    "- Want to see decision rules\n",
    "- Educational purposes\n",
    "- Small datasets\n",
    "\n",
    "**Random Forest:**\n",
    "- Need highest accuracy\n",
    "- Have sufficient data\n",
    "- Production deployment\n",
    "- Don't need to explain individual predictions\n",
    "\n",
    "### Next Steps for Team\n",
    "1. Deploy best model to production\n",
    "2. Monitor performance on new data\n",
    "3. Retrain periodically with fresh data\n",
    "4. Consider ensembling multiple models\n",
    "5. Experiment with advanced techniques (XGBoost, Neural Networks)\n",
    "\n",
    "### Player Statistics Models\n",
    "‚Üí Notebook 07: Build regression models for predicting player stats!\n",
    "\n",
    "üèÜ Model comparison complete - ready for deployment!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
